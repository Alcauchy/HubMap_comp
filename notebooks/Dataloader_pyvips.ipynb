{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tifffile\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pyvips\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../dataset/'\n",
    "train_folder = f'{dataset_folder}train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_dtype = {\n",
    "    'uchar': np.uint8,\n",
    "    'char': np.int8,\n",
    "    'ushort': np.uint16,\n",
    "    'short': np.int16,\n",
    "    'uint': np.uint32,\n",
    "    'int': np.int32,\n",
    "    'float': np.float32,\n",
    "    'double': np.float64,\n",
    "    'complex': np.complex64,\n",
    "    'dpcomplex': np.complex128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vips2numpy(vi):\n",
    "    return np.ndarray(buffer=vi.write_to_memory(),\n",
    "                      dtype=format_to_dtype[vi.format],\n",
    "                      shape=[vi.height, vi.width, vi.bands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = []\n",
    "masks_paths = []\n",
    "for item in os.listdir(train_folder):\n",
    "    if '.tiff' not in item:\n",
    "        continue\n",
    "    if '_mask' in item:\n",
    "        masks_paths.append(train_folder + item)\n",
    "    else:\n",
    "        images_paths.append(train_folder + item)\n",
    "images_paths = sorted(images_paths)\n",
    "masks_paths = sorted(masks_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "## Check correspondense\n",
    "if len(images_paths) != len(masks_paths):\n",
    "    raise ValueError('Lists have different sizes')\n",
    "    \n",
    "for image, mask in zip(images_paths, masks_paths):\n",
    "    if image != mask.replace('_mask', ''):\n",
    "        raise(f'{image} has no mask or masks is in wrong order')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image, patch_size, x, y):\n",
    "    \"\"\"\n",
    "    Crop  square form pyvips image\n",
    "    :param image: pyvips image.\n",
    "    :param patch_size: size of square to be croped from image .\n",
    "    :param x: x coordinate of top left point.\n",
    "    :param y: y coordinate of top left point.\n",
    "    :return: return croped pyvips image.\n",
    "    \"\"\"\n",
    "    return image.crop(patch_size * x, patch_size * y, patch_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(path, patch_size, padding=True):\n",
    "    \"\"\"\n",
    "    Split image by patch_size without loading it to RAM.\n",
    "    :param path: Path to image.\n",
    "    :param patch_size: Size of square to be splitted.\n",
    "    :param padding: If True make morror padding.\n",
    "    :return: List of pyvips cropes and ids for fetch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    ids = []\n",
    "    img = pyvips.Image.new_from_file(path)\n",
    "    if padding:\n",
    "        new_width = int(np.ceil(img.width / patch_size)) * patch_size\n",
    "        new_height = int(np.ceil(img.height / patch_size)) * patch_size\n",
    "        img = img.embed(0, 0, new_width, new_height, extend='mirror')\n",
    "    else:\n",
    "        new_width = img.width\n",
    "        new_height = img.height\n",
    "        \n",
    "    x_num = new_width // patch_size\n",
    "    y_num = new_height // patch_size\n",
    "    for y in range(y_num):\n",
    "        for x in range(x_num):\n",
    "            patches.append(crop(img, patch_size, x, y))\n",
    "            ids.append((path, x, y))\n",
    "    return patches, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "image_ids = []\n",
    "for mask_path in masks_paths:\n",
    "    result = split(mask_path, patch_size=patch_size)\n",
    "    masks += result[0]\n",
    "    image_ids += result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_with_object = np.zeros(len(masks)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_object(item):\n",
    "    return int(np.sum(vips2numpy(item)) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12345/12345 [03:00<00:00, 68.58it/s] \n"
     ]
    }
   ],
   "source": [
    "for i, item in tqdm(enumerate(masks), total=len(masks)):\n",
    "    is_with_object[i] = check_object(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10104.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,  2241.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAEHJJREFUeJzt3H+s3Xddx/Hny5bNAQ461i2zHbZoBbpFAquzghJ0Jitg7ExYUhTWkCWNcyIaE+n4w5mYJltiFBfcyDJwnRJqMxZXhaFLcaJhbN7BoOvqXGWzq6vr5YcwMQ473v5xPiSHfm7bs3vuvae3fT6Sk/P9vr+fz/d8PmlzXvf743xTVUiSNOwHJj0ASdLJx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSZ+mkBzBb5557bq1atWrSw5CkReWhhx76alUtP1G7RRsOq1atYmpqatLDkKRFJcm/j9LO00qSpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpM4JwyHJR5McTvLIUO2cJPcmeby9Lxvadl2S/UkeS3L5UP2SJHvatpuSpNXPTPKXrf5AklVzO0VJ0gs1ypHD7cCGo2pbgd1VtQbY3dZJshbYBFzU+tycZEnrcwuwBVjTXt/b59XAN6rqx4A/Bm6c7WQkSXPjhL+QrqrPzvDX/EbgLW15O3Af8P5W31FVzwFPJNkPXJrkSeDsqrofIMkdwBXAPa3P77d93Ql8KEmqqmY7qRNZtfWT87XrE3ryhrdP7LMlaVSzveZwflUdAmjv57X6CuCpoXYHW21FWz66/n19quoI8E3gFbMclyRpDsz1BenMUKvj1I/Xp995siXJVJKp6enpWQ5RknQisw2HZ5JcANDeD7f6QeDCoXYrgadbfeUM9e/rk2Qp8DLg6zN9aFXdWlXrqmrd8uUnfKigJGmWZhsOu4DNbXkzcPdQfVO7A2k1gwvPD7ZTT88mWd/uUrrqqD7f29c7gM/M5/UGSdKJnfCCdJKPM7j4fG6Sg8D1wA3AziRXAweAKwGqam+SncCjwBHg2qp6vu3qGgZ3Pp3F4EL0Pa3+EeDP28XrrzO420mSNEGj3K30zmNsuuwY7bcB22aoTwEXz1D/X1q4SJJODv5CWpLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSZ2xwiHJbyfZm+SRJB9P8oNJzklyb5LH2/uyofbXJdmf5LEklw/VL0myp227KUnGGZckaTyzDockK4DfBNZV1cXAEmATsBXYXVVrgN1tnSRr2/aLgA3AzUmWtN3dAmwB1rTXhtmOS5I0vnFPKy0FzkqyFHgx8DSwEdjetm8HrmjLG4EdVfVcVT0B7AcuTXIBcHZV3V9VBdwx1EeSNAGzDoeq+g/gD4EDwCHgm1X1d8D5VXWotTkEnNe6rACeGtrFwVZb0ZaPrkuSJmSc00rLGBwNrAZ+GHhJkncdr8sMtTpOfabP3JJkKsnU9PT0Cx2yJGlE45xW+gXgiaqarqr/A+4C3gg8004V0d4Pt/YHgQuH+q9kcBrqYFs+ut6pqlural1VrVu+fPkYQ5ckHc844XAAWJ/kxe3uosuAfcAuYHNrsxm4uy3vAjYlOTPJagYXnh9sp56eTbK+7eeqoT6SpAlYOtuOVfVAkjuBLwBHgC8CtwIvBXYmuZpBgFzZ2u9NshN4tLW/tqqeb7u7BrgdOAu4p70kSRMy63AAqKrrgeuPKj/H4ChipvbbgG0z1KeAi8cZiyRp7vgLaUlSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHXGCockL09yZ5J/SbIvyU8nOSfJvUkeb+/Lhtpfl2R/kseSXD5UvyTJnrbtpiQZZ1ySpPGMe+TwJ8Cnq+o1wOuAfcBWYHdVrQF2t3WSrAU2ARcBG4Cbkyxp+7kF2AKsaa8NY45LkjSGWYdDkrOBNwMfAaiq71TVfwEbge2t2Xbgira8EdhRVc9V1RPAfuDSJBcAZ1fV/VVVwB1DfSRJEzDOkcOrgGngz5J8McltSV4CnF9VhwDa+3mt/QrgqaH+B1ttRVs+ui5JmpBxwmEp8Abglqp6PfBt2imkY5jpOkIdp97vINmSZCrJ1PT09AsdryRpROOEw0HgYFU90NbvZBAWz7RTRbT3w0PtLxzqvxJ4utVXzlDvVNWtVbWuqtYtX758jKFLko5n1uFQVf8JPJXk1a10GfAosAvY3Gqbgbvb8i5gU5Izk6xmcOH5wXbq6dkk69tdSlcN9ZEkTcDSMfu/F/hYkjOArwDvYRA4O5NcDRwArgSoqr1JdjIIkCPAtVX1fNvPNcDtwFnAPe0lSZqQscKhqh4G1s2w6bJjtN8GbJuhPgVcPM5YJElzx19IS5I6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqTN2OCRZkuSLSf6mrZ+T5N4kj7f3ZUNtr0uyP8ljSS4fql+SZE/bdlOSjDsuSdLszcWRw/uAfUPrW4HdVbUG2N3WSbIW2ARcBGwAbk6ypPW5BdgCrGmvDXMwLknSLI0VDklWAm8HbhsqbwS2t+XtwBVD9R1V9VxVPQHsBy5NcgFwdlXdX1UF3DHUR5I0AeMeOXwQ+F3gu0O186vqEEB7P6/VVwBPDbU72Gor2vLRdUnShMw6HJL8InC4qh4atcsMtTpOfabP3JJkKsnU9PT0iB8rSXqhxjlyeBPwS0meBHYAP5/kL4Bn2qki2vvh1v4gcOFQ/5XA062+coZ6p6purap1VbVu+fLlYwxdknQ8sw6HqrquqlZW1SoGF5o/U1XvAnYBm1uzzcDdbXkXsCnJmUlWM7jw/GA79fRskvXtLqWrhvpIkiZg6Tzs8wZgZ5KrgQPAlQBVtTfJTuBR4AhwbVU93/pcA9wOnAXc016SpAmZk3CoqvuA+9ry14DLjtFuG7BthvoUcPFcjEWSND5/IS1J6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6iyd9AAkaTFatfWTE/vsJ294+7x/xqyPHJJcmOTvk+xLsjfJ+1r9nCT3Jnm8vS8b6nNdkv1JHkty+VD9kiR72rabkmS8aUmSxjHOaaUjwO9U1WuB9cC1SdYCW4HdVbUG2N3Wads2ARcBG4Cbkyxp+7oF2AKsaa8NY4xLkjSmWYdDVR2qqi+05WeBfcAKYCOwvTXbDlzRljcCO6rquap6AtgPXJrkAuDsqrq/qgq4Y6iPJGkC5uSCdJJVwOuBB4Dzq+oQDAIEOK81WwE8NdTtYKutaMtH1yVJEzJ2OCR5KfAJ4Leq6lvHazpDrY5Tn+mztiSZSjI1PT39wgcrSRrJWOGQ5EUMguFjVXVXKz/TThXR3g+3+kHgwqHuK4GnW33lDPVOVd1aVeuqat3y5cvHGbok6TjGuVspwEeAfVX1R0ObdgGb2/Jm4O6h+qYkZyZZzeDC84Pt1NOzSda3fV411EeSNAHj/M7hTcC7gT1JHm61DwA3ADuTXA0cAK4EqKq9SXYCjzK40+naqnq+9bsGuB04C7invSRJEzLrcKiqf2Lm6wUAlx2jzzZg2wz1KeDi2Y5FkjS3fHyGJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOidNOCTZkOSxJPuTbJ30eCTpdHZShEOSJcCfAm8F1gLvTLJ2sqOSpNPXSREOwKXA/qr6SlV9B9gBbJzwmCTptHWyhMMK4Kmh9YOtJkmagKWTHkCTGWrVNUq2AFva6n8neWyWn3cu8NVZ9h1LbpzEpwITnPMEOefTw2k359w41px/ZJRGJ0s4HAQuHFpfCTx9dKOquhW4ddwPSzJVVevG3c9i4pxPD8759LAQcz5ZTiv9M7AmyeokZwCbgF0THpMknbZOiiOHqjqS5DeAvwWWAB+tqr0THpYknbZOinAAqKpPAZ9aoI8b+9TUIuScTw/O+fQw73NOVXfdV5J0mjtZrjlIkk4ip3Q4nOiRHBm4qW3/cpI3TGKcc2mEOf9qm+uXk3wuyesmMc65NOqjV5L8ZJLnk7xjIcc3H0aZc5K3JHk4yd4k/7DQY5xLI/y/flmSv07ypTbf90xinHMpyUeTHE7yyDG2z+/3V1Wdki8GF7b/DXgVcAbwJWDtUW3eBtzD4HcW64EHJj3uBZjzG4Flbfmtp8Och9p9hsF1rXdMetwL8O/8cuBR4JVt/bxJj3ue5/sB4Ma2vBz4OnDGpMc+5rzfDLwBeOQY2+f1++tUPnIY5ZEcG4E7auDzwMuTXLDQA51DJ5xzVX2uqr7RVj/P4Dcli9moj155L/AJ4PBCDm6ejDLnXwHuqqoDAFW1mOc9ynwL+KEkAV7KIByOLOww51ZVfZbBPI5lXr+/TuVwGOWRHKfaYzte6HyuZvCXx2J2wjknWQH8MvDhBRzXfBrl3/nHgWVJ7kvyUJKrFmx0c2+U+X4IeC2DH8/uAd5XVd9dmOFNzLx+f500t7LOg1EeyTHSYzsWkZHnk+TnGITDz8zriObfKHP+IPD+qnp+8IflojfKnJcClwCXAWcB9yf5fFX963wPbh6MMt/LgYeBnwd+FLg3yT9W1bfme3ATNK/fX6dyOIzySI6RHtuxiIw0nyQ/AdwGvLWqvrZAY5svo8x5HbCjBcO5wNuSHKmqv1qYIc65Uf9vf7Wqvg18O8lngdcBizEcRpnve4AbanAyfn+SJ4DXAA8uzBAnYl6/v07l00qjPJJjF3BVu+q/HvhmVR1a6IHOoRPOOckrgbuAdy/SvyKPdsI5V9XqqlpVVauAO4FfX8TBAKP9374b+NkkS5O8GPgpYN8Cj3OujDLfAwyOkkhyPvBq4CsLOsqFN6/fX6fskUMd45EcSX6tbf8wgztX3gbsB/6HwV8fi9aIc/494BXAze0v6SO1iB9aNuKcTymjzLmq9iX5NPBl4LvAbVU14y2RJ7sR/43/ALg9yR4Gp1veX1WL+kmtST4OvAU4N8lB4HrgRbAw31/+QlqS1DmVTytJkmbJcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdf4fYAWzrdl8D+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(is_with_object.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dicts and label paths\n",
    "mask2ind = {}\n",
    "image2ind = {}\n",
    "ind2mask = {}\n",
    "ind2image = {}\n",
    "dataset = np.array(image_ids)\n",
    "uniq_mask_path = np.unique(dataset[:,0])\n",
    "for i, item in enumerate(uniq_mask_path):\n",
    "    mask2ind[item] = i\n",
    "    ind2mask[i] = item\n",
    "    image2ind[item.replace('_mask', '')] = i\n",
    "    ind2image[i] = item.replace('_mask', '')\n",
    "dataset[:, 0] = np.vectorize(mask2ind.get)(dataset[:, 0])\n",
    "dataset = dataset.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(dataset, \n",
    "                 train_size=0.8, \n",
    "                 test_size=0.2, \n",
    "                 stratify=is_with_object.squeeze(),\n",
    "                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubDataset(Dataset):\n",
    "    def __init__(self, dataset, patch_size, ind2image, ind2mask, transform=None):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.patch_size = patch_size\n",
    "        self.ind2image = ind2image\n",
    "        self.ind2mask = ind2mask\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = self.transform_default\n",
    "        self.images = self.load_images()\n",
    "        self.masks = self.load_masks()\n",
    "        \n",
    "        \n",
    "    def load_images(self):\n",
    "        pass\n",
    "    \n",
    "    def load_masks(self):\n",
    "        pass\n",
    "    \n",
    "    def transform_default(self):\n",
    "        pass\n",
    "    \n",
    "    def get_image(self, image_objct, x, y):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ind, patch_size, x, y = self.dataset[idx]\n",
    "        image = self.get_image(self.images[ind], x, y)\n",
    "        mask = self.get_image(self.masks[ind], x, y)\n",
    "        aug = self.transform(image=image, mask=mask)\n",
    "        image = aug['image']\n",
    "        mask = aug['mask']\n",
    "        image = torch.Tensor(image.permute(2, 0, 1))\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "venv_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
