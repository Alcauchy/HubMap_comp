{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:31:47.677980Z",
     "start_time": "2020-12-22T16:31:45.641810Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import cv2\n",
    "import tifffile\n",
    "import skimage\n",
    "from skimage import io, transform\n",
    "from scipy import ndimage as nd\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import json\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.unet import Unet\n",
    "import sys\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import time\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:31:48.544377Z",
     "start_time": "2020-12-22T16:31:48.380342Z"
    }
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose,\n",
    "    OneOf,\n",
    "    Flip,\n",
    "    Rotate,\n",
    "    RandomRotate90,\n",
    "    CLAHE,\n",
    "    RandomBrightnessContrast,\n",
    "    RandomGamma,\n",
    "    GaussianBlur,\n",
    "    GaussNoise,\n",
    "    RandomCrop,\n",
    "    ShiftScaleRotate,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    Normalize,\n",
    "    RandomCrop,\n",
    "    RandomScale,\n",
    "    OpticalDistortion,\n",
    "    ElasticTransform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_PATH = \"./\"\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n",
    "DATA_PATH = os.path.join(BASE_PATH,\"processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:31:49.736315Z",
     "start_time": "2020-12-22T16:31:49.729811Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_kidney(image, mask, alpha = 0.5):\n",
    "    \"\"\"Show image with mask\"\"\"\n",
    "    #if image.shape[0] == 0 \n",
    "    if type(image) == torch.Tensor:\n",
    "        plt.imshow(image.permute(1,2,0))\n",
    "    #if type(image) == nd.array:\n",
    "       # plt.imshow(image.transpose(1,2,0))\n",
    "    plt.imshow(mask[0,...].numpy(), cmap=\"hot\", alpha=alpha)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:31:53.561644Z",
     "start_time": "2020-12-22T16:31:53.489002Z"
    }
   },
   "outputs": [],
   "source": [
    "# tutorial can be found here:\n",
    "# https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html\n",
    "# class to load one pic\n",
    "class KidneyFTUsDataset(Dataset):\n",
    "    '''\n",
    "    Kdiney with FTU mask dataset\n",
    "    '''\n",
    "    def __init__(self, root_dir, csv_file, mask_file, name_df = None, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with pic names.\n",
    "            mask_file (string): hdf5 file containing masks \n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        if name_df is not None:\n",
    "            self.name_list = name_df\n",
    "        else:\n",
    "            self.name_list = pd.read_csv(os.path.join(self.root_dir, csv_file))\n",
    "        self.mask_file = h5py.File(os.path.join(self.root_dir, mask_file), 'r')\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = self.__transform_default()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "            tile_name = self.name_list.iloc[idx]['name']\n",
    "            mask = self.mask_file[tile_name][()]\n",
    "            mask = skimage.transform.resize(mask,(256,256))\n",
    "            image = tifffile.imread(os.path.join(self.root_dir, \n",
    "                                                 tile_name + '.tiff'))\n",
    "            image = cv2.resize(image,(256,256))\n",
    "            \n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "            image = torch.Tensor(image).permute(2, 0, 1)\n",
    "            mask = torch.Tensor(mask).unsqueeze(0)\n",
    "            \n",
    "            sample = {'image': image, 'mask': mask}\n",
    "            return sample\n",
    "        \n",
    "    def __transform_default(self):\n",
    "        return Compose([\n",
    "            Normalize(max_pixel_value=255.0),\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:31:54.958090Z",
     "start_time": "2020-12-22T16:31:54.951702Z"
    }
   },
   "outputs": [],
   "source": [
    "aug_train = Compose([Flip(p=0.5),\n",
    "                     RandomRotate90(p=0.5),\n",
    "                     Rotate(limit=180, p=0.5),\n",
    "                     Normalize(max_pixel_value=255.0),\n",
    "        ])\n",
    "aug_valid = Compose([Normalize(max_pixel_value=255.0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuBMAP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HuBMAP, self).__init__()\n",
    "        self.cnn_model = Unet('efficientnet-b5', encoder_weights=\"imagenet\", classes=1, activation=None)\n",
    "        #self.cnn_model.decoder.blocks.append(self.cnn_model.decoder.blocks[-1])\n",
    "        #self.cnn_model.decoder.blocks[-2] = self.cnn_model.decoder.blocks[-3]\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_segs = self.cnn_model(imgs)\n",
    "        return img_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_valid_dataloader(df, fold):\n",
    "    train_ids = df.loc[~df.Folds.isin(fold)]\n",
    "    val_ids = df.loc[df.Folds.isin(fold)]\n",
    "    train_ds = KidneyFTUsDataset(root_dir = DATA_PATH, \n",
    "                                        csv_file = 'names.csv', \n",
    "                                        mask_file = 'mask.h5',\n",
    "                                        name_df = train_ids,\n",
    "                                        transform=aug_train)\n",
    "    val_ds = KidneyFTUsDataset(root_dir = DATA_PATH, \n",
    "                                        csv_file = 'names.csv', \n",
    "                                        mask_file = 'mask.h5',\n",
    "                                        name_df = val_ids,\n",
    "                                        transform=aug_train)\n",
    "    train_loader = DataLoader(train_ds, batch_size=12, \n",
    "                              pin_memory=True, shuffle=True, \n",
    "                              num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4, \n",
    "                            pin_memory=True, shuffle=False, \n",
    "                            num_workers=4)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:33:22.858449Z",
     "start_time": "2020-12-22T16:33:22.809445Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/vineeth1999/hubmap-pytorch-efficientunet-offline\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return dice\n",
    "\n",
    "class DiceBCELoss(torch.nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = 0.1*BCE + 0.9*dice_loss\n",
    "        \n",
    "        return Dice_BCE.mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:33:25.644216Z",
     "start_time": "2020-12-22T16:33:25.634433Z"
    }
   },
   "outputs": [],
   "source": [
    "def HuBMAPLoss(images, targets, model, device):\n",
    "    model.to(device)\n",
    "    print('transferring images to gpu')\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(images)\n",
    "    criterion = DiceBCELoss()\n",
    "    loss = criterion(outputs, targets)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:33:27.448681Z",
     "start_time": "2020-12-22T16:33:27.397255Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader):\n",
    "    model.train()\n",
    "    t = time.time()\n",
    "    total_loss = 0\n",
    "    for step, dic in enumerate(trainloader):\n",
    "        loss, outputs = HuBMAPLoss(dic['image'], dic['mask'], model, device)\n",
    "        loss.backward()\n",
    "        if ((step+1)%4==0 or (step+1)==len(trainloader)):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        loss = loss.detach().item()\n",
    "        total_loss += loss\n",
    "        if ((step+1)%1==0 or (step+1)==len(trainloader)):\n",
    "            print(\n",
    "                    f'epoch {epoch} train step {step+1}/{len(trainloader)}, ' + \\\n",
    "                    f'loss: {total_loss/len(trainloader):.4f}, ' + \\\n",
    "                    f'time: {(time.time() - t):.4f}', end= '\\r' if (step + 1) != len(trainloader) else '\\n'\n",
    "                )\n",
    "\n",
    "            \n",
    "        \n",
    "def valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader):\n",
    "    model.eval()\n",
    "    t = time.time()\n",
    "    total_loss = 0\n",
    "    for step, dic in enumerate(validloader):\n",
    "        loss, outputs = HuBMAPLoss(dic['image'], dic['mask'], model, device)\n",
    "        loss = loss.detach().item()\n",
    "        total_loss += loss\n",
    "        if ((step+1)%4==0 or (step+1)==len(validloader)):\n",
    "            scheduler.step(total_loss/len(validloader))\n",
    "        if ((step+1)%10==0 or (step+1)==len(validloader)):\n",
    "            print(\n",
    "                    f'epoch {epoch} trainz step {step+1}/{len(validloader)}, ' + \\\n",
    "                    f'loss: {total_loss/len(validloader):.4f}, ' + \\\n",
    "                    f'time: {(time.time() - t):.4f}', end= '\\r' if (step + 1) != len(validloader) else '\\n'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_df = pd.read_csv(os.path.join(DATA_PATH, 'names.csv'))\n",
    "dir_df['Folds'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "gkf = GroupKFold(FOLDS)\n",
    "dir_df['Folds'] = 0\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):\n",
    "    dir_df.loc[val_idx, 'Folds'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):\n",
    "    if fold>0:\n",
    "        break\n",
    "    \n",
    "    trainloader, validloader = prepare_train_valid_dataloader(dir_df, [fold])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = HuBMAP().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=1)\n",
    "    num_epochs = 15\n",
    "    #num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader)\n",
    "    torch.save(model.state_dict(),os.path.join('./weights',f'FOLD-{fold}-model.pth'))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T16:27:35.296527Z",
     "start_time": "2020-12-22T16:27:35.176439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loading_tutorial.ipynb\t   HubMap.ipynb\t\t  train\r\n",
      "data_prep\t\t\t   processed\t\t  train.csv\r\n",
      "FOLD-0-model.pth\t\t   sample_submission.csv  Untitled.ipynb\r\n",
      "HuBMAP-20-dataset_information.csv  test\t\t\t  weights\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
